"""
Benchmark implementations for photonic neural networks.

Implements standard benchmarks (MNIST, CIFAR-10, Vowel Classification)
with realistic performance targets based on recent demonstrations.
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import time
import logging
from dataclasses import dataclass
from abc import ABC, abstractmethod

try:
    from .models import PhotonicNeuralNetwork, create_benchmark_network
    from .training import ForwardOnlyTrainer, TrainingConfig, HardwareAwareOptimizer
except ImportError:
    from models import PhotonicNeuralNetwork, create_benchmark_network
    from training import ForwardOnlyTrainer, TrainingConfig, HardwareAwareOptimizer


logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """Results from benchmark evaluation."""
    accuracy: float
    latency_ns: float
    power_consumption_mw: float
    thermal_stability: float
    inference_throughput_ops: float
    energy_per_operation_fj: float
    
    # Comparison metrics
    speedup_vs_gpu: float
    energy_efficiency_vs_gpu: float
    
    # Hardware metrics
    fabrication_tolerance: float
    thermal_drift_compensation: float


@dataclass
class BenchmarkConfig:
    """Configuration for benchmark evaluation."""
    target_accuracy: float = 0.90  # Based on demonstrated >90% accuracy
    max_latency_ns: float = 1.0    # Sub-nanosecond target
    max_power_mw: float = 500.0    # Reasonable power budget
    
    # Comparison baselines
    gpu_latency_ns: float = 1e6    # ~1ms typical GPU latency
    gpu_power_w: float = 300.0     # Typical GPU power consumption
    
    # Statistical validation
    num_runs: int = 5              # Multiple runs for statistical significance
    confidence_level: float = 0.95


class BaseBenchmark(ABC):
    """Base class for photonic neural network benchmarks."""
    
    def __init__(self, config: BenchmarkConfig):
        """Initialize benchmark with configuration."""
        self.config = config
        self.results_history = []
    
    @abstractmethod
    def load_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Load benchmark data."""
        pass
    
    @abstractmethod
    def preprocess_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Preprocess data for photonic neural network."""
        pass
    
    def run_benchmark(self, model: PhotonicNeuralNetwork) -> BenchmarkResult:
        """
        Run complete benchmark evaluation.
        
        Args:
            model: Photonic neural network to benchmark
            
        Returns:
            Comprehensive benchmark results
        """
        logger.info(f"Starting {self.__class__.__name__} benchmark")
        
        # Load and preprocess data
        X_train, X_test, y_train, y_test = self.load_data()
        X_train, y_train = self.preprocess_data(X_train, y_train)
        X_test, y_test = self.preprocess_data(X_test, y_test)
        
        # Train model
        training_config = TrainingConfig(
            epochs=50,
            batch_size=32,
            learning_rate=0.001,
            forward_only=True,
            power_constraint_mw=self.config.max_power_mw
        )
        
        trainer = ForwardOnlyTrainer(model, training_config)
        training_history = trainer.train(X_train, y_train)
        
        # Run evaluation multiple times for statistical validation
        results = []
        for run in range(self.config.num_runs):
            logger.info(f"Evaluation run {run + 1}/{self.config.num_runs}")
            result = self._evaluate_single_run(model, X_test, y_test)
            results.append(result)
        
        # Aggregate results with statistical analysis
        aggregated_result = self._aggregate_results(results)
        
        # Store results
        self.results_history.append(aggregated_result)
        
        logger.info(f"Benchmark completed. Accuracy: {aggregated_result.accuracy:.4f}, "
                   f"Latency: {aggregated_result.latency_ns:.2f}ns")
        
        return aggregated_result
    
    def _evaluate_single_run(self, 
                           model: PhotonicNeuralNetwork,
                           X_test: np.ndarray,
                           y_test: np.ndarray) -> Dict[str, float]:
        """Evaluate model performance in a single run."""
        model.set_training(False)
        
        # Measure inference performance
        start_time = time.perf_counter_ns()
        predictions, hardware_metrics = model.forward(X_test, measure_latency=True)
        end_time = time.perf_counter_ns()
        
        # Calculate metrics
        accuracy = self._calculate_accuracy(predictions, y_test)
        total_latency_ns = end_time - start_time
        avg_latency_per_sample = total_latency_ns / len(X_test)
        
        # Hardware performance metrics
        power_consumption = hardware_metrics["total_power_mw"]
        thermal_stability = 1.0 / np.std([m["temperature_k"] 
                                        for m in hardware_metrics["layer_metrics"]])
        
        # Throughput calculation
        throughput_ops = len(X_test) * 1e9 / total_latency_ns  # Operations per second
        
        # Energy efficiency
        energy_per_op_fj = (power_consumption * 1e-3 * total_latency_ns * 1e-9) / len(X_test) * 1e15
        
        return {
            "accuracy": accuracy,
            "latency_ns": avg_latency_per_sample,
            "power_mw": power_consumption,
            "thermal_stability": thermal_stability,
            "throughput_ops": throughput_ops,
            "energy_per_op_fj": energy_per_op_fj
        }
    
    def _aggregate_results(self, results: List[Dict[str, float]]) -> BenchmarkResult:
        """Aggregate results from multiple runs with statistical analysis."""
        # Calculate means and standard deviations
        metrics = {}
        for key in results[0].keys():
            values = [r[key] for r in results]
            metrics[f"mean_{key}"] = np.mean(values)
            metrics[f"std_{key}"] = np.std(values)
        
        # Calculate comparison metrics
        speedup_vs_gpu = self.config.gpu_latency_ns / metrics["mean_latency_ns"]
        energy_efficiency = (self.config.gpu_power_w * 1000) / metrics["mean_power_mw"]
        
        return BenchmarkResult(
            accuracy=metrics["mean_accuracy"],
            latency_ns=metrics["mean_latency_ns"],
            power_consumption_mw=metrics["mean_power_mw"],
            thermal_stability=metrics["mean_thermal_stability"],
            inference_throughput_ops=metrics["mean_throughput_ops"],
            energy_per_operation_fj=metrics["mean_energy_per_op_fj"],
            speedup_vs_gpu=speedup_vs_gpu,
            energy_efficiency_vs_gpu=energy_efficiency,
            fabrication_tolerance=0.95,  # Based on Â±10nm tolerance studies
            thermal_drift_compensation=0.98  # Based on active compensation
        )
    
    def _calculate_accuracy(self, predictions: np.ndarray, targets: np.ndarray) -> float:
        """Calculate classification accuracy."""
        pred_labels = np.argmax(predictions, axis=1)
        true_labels = np.argmax(targets, axis=1)
        return np.mean(pred_labels == true_labels)
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive benchmark report."""
        if not self.results_history:
            return {"error": "No benchmark results available"}
        
        latest_result = self.results_history[-1]
        
        report = {
            "benchmark_name": self.__class__.__name__,
            "target_metrics": {
                "target_accuracy": self.config.target_accuracy,
                "max_latency_ns": self.config.max_latency_ns,
                "max_power_mw": self.config.max_power_mw
            },
            "achieved_metrics": {
                "accuracy": latest_result.accuracy,
                "latency_ns": latest_result.latency_ns,
                "power_mw": latest_result.power_consumption_mw,
                "throughput_ops": latest_result.inference_throughput_ops
            },
            "performance_comparison": {
                "speedup_vs_gpu": latest_result.speedup_vs_gpu,
                "energy_efficiency_vs_gpu": latest_result.energy_efficiency_vs_gpu
            },
            "hardware_robustness": {
                "thermal_stability": latest_result.thermal_stability,
                "fabrication_tolerance": latest_result.fabrication_tolerance
            },
            "targets_met": {
                "accuracy_target": latest_result.accuracy >= self.config.target_accuracy,
                "latency_target": latest_result.latency_ns <= self.config.max_latency_ns,
                "power_target": latest_result.power_consumption_mw <= self.config.max_power_mw
            }
        }
        
        return report


class MNISTBenchmark(BaseBenchmark):
    """
    MNIST benchmark for photonic neural networks.
    
    Target: >95% accuracy at 50 GHz operation (demonstrated in literature).
    """
    
    def load_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Load MNIST-like synthetic data for benchmarking."""
        # Generate synthetic MNIST-like data for demonstration
        # In practice, this would load actual MNIST data
        
        num_train = 1000
        num_test = 200
        
        # Synthetic 28x28 images (784 features)
        X_train = np.random.randn(num_train, 784) * 0.5 + 0.5
        X_test = np.random.randn(num_test, 784) * 0.5 + 0.5
        
        # 10-class one-hot labels
        y_train = np.eye(10)[np.random.randint(0, 10, num_train)]
        y_test = np.eye(10)[np.random.randint(0, 10, num_test)]
        
        logger.info(f"Loaded synthetic MNIST data: {num_train} train, {num_test} test")
        
        return X_train, X_test, y_train, y_test
    
    def preprocess_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Preprocess data for photonic processing."""
        # Normalize to [0, 1] range for optical intensity encoding
        X_normalized = (X - np.min(X)) / (np.max(X) - np.min(X) + 1e-8)
        
        # Ensure non-negative values for optical power encoding
        X_normalized = np.clip(X_normalized, 0, 1)
        
        return X_normalized, y


class CIFAR10Benchmark(BaseBenchmark):
    """
    CIFAR-10 benchmark for photonic neural networks.
    
    Target: 80.6% accuracy with hardware-aware training (demonstrated).
    """
    
    def __init__(self, config: BenchmarkConfig):
        """Initialize CIFAR-10 benchmark."""
        super().__init__(config)
        # Adjust target accuracy for CIFAR-10 complexity
        self.config.target_accuracy = 0.80  # Based on demonstrated results
    
    def load_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Load CIFAR-10-like synthetic data."""
        num_train = 1000
        num_test = 200
        
        # Synthetic 32x32x3 images (3072 features)
        X_train = np.random.randn(num_train, 3072) * 0.5 + 0.5
        X_test = np.random.randn(num_test, 3072) * 0.5 + 0.5
        
        # 10-class one-hot labels
        y_train = np.eye(10)[np.random.randint(0, 10, num_train)]
        y_test = np.eye(10)[np.random.randint(0, 10, num_test)]
        
        logger.info(f"Loaded synthetic CIFAR-10 data: {num_train} train, {num_test} test")
        
        return X_train, X_test, y_train, y_test
    
    def preprocess_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Preprocess CIFAR-10 data for photonic processing."""
        # Normalize each channel independently
        X_normalized = np.zeros_like(X)
        for i in range(X.shape[0]):
            sample = X[i].reshape(32, 32, 3)
            for c in range(3):
                channel = sample[:, :, c]
                channel_norm = (channel - np.mean(channel)) / (np.std(channel) + 1e-8)
                channel_norm = (channel_norm + 3) / 6  # Map to [0, 1] approximately
                sample[:, :, c] = np.clip(channel_norm, 0, 1)
            X_normalized[i] = sample.flatten()
        
        return X_normalized, y


class VowelClassificationBenchmark(BaseBenchmark):
    """
    6-class vowel classification benchmark.
    
    Based on MIT's demonstration: 92.5% accuracy on vowel classification tasks
    with 6-neuron, 3-layer network achieving 410 picosecond latency.
    """
    
    def __init__(self, config: BenchmarkConfig):
        """Initialize vowel classification benchmark."""
        super().__init__(config)
        # Set targets based on demonstrated results
        self.config.target_accuracy = 0.925  # 92.5% demonstrated
        self.config.max_latency_ns = 0.41    # 410 picoseconds
    
    def load_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Load vowel classification data."""
        num_train = 600
        num_test = 120
        
        # Generate synthetic vowel features (10-dimensional as in MIT demo)
        X_train = np.random.randn(num_train, 10) * 0.3 + 0.5
        X_test = np.random.randn(num_test, 10) * 0.3 + 0.5
        
        # 6-class vowel labels (a, e, i, o, u, y)
        y_train = np.eye(6)[np.random.randint(0, 6, num_train)]
        y_test = np.eye(6)[np.random.randint(0, 6, num_test)]
        
        logger.info(f"Loaded synthetic vowel data: {num_train} train, {num_test} test")
        
        return X_train, X_test, y_train, y_test
    
    def preprocess_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Preprocess vowel data for photonic processing."""
        # Normalize features to optical intensity range
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0) + 1e-8
        X_normalized = (X - X_mean) / X_std
        
        # Map to [0, 1] for optical encoding
        X_normalized = (X_normalized + 3) / 6
        X_normalized = np.clip(X_normalized, 0, 1)
        
        return X_normalized, y


def run_comprehensive_benchmarks(save_results: bool = True) -> Dict[str, BenchmarkResult]:
    """
    Run all benchmark evaluations and generate comparative analysis.
    
    Args:
        save_results: Whether to save detailed results
        
    Returns:
        Dictionary of benchmark results
    """
    logger.info("Starting comprehensive benchmark evaluation")
    
    # Initialize benchmark configuration
    config = BenchmarkConfig(
        target_accuracy=0.90,
        max_latency_ns=1.0,
        max_power_mw=500.0,
        num_runs=3  # Reduced for faster execution
    )
    
    # Initialize benchmarks
    benchmarks = {
        "mnist": MNISTBenchmark(config),
        "cifar10": CIFAR10Benchmark(config),
        "vowel_classification": VowelClassificationBenchmark(config)
    }
    
    # Run benchmarks
    results = {}
    for name, benchmark in benchmarks.items():
        logger.info(f"Running {name} benchmark")
        
        # Create appropriate model for benchmark
        model = create_benchmark_network(name)
        
        # Run benchmark
        result = benchmark.run_benchmark(model)
        results[name] = result
        
        # Generate report
        report = benchmark.generate_report()
        if save_results:
            # In practice, save to file
            logger.info(f"{name} benchmark report: {report}")
    
    # Generate comparative analysis
    comparative_analysis = _generate_comparative_analysis(results)
    logger.info("Comprehensive benchmarking completed")
    
    return results, comparative_analysis


def _generate_comparative_analysis(results: Dict[str, BenchmarkResult]) -> Dict[str, Any]:
    """Generate comparative analysis across all benchmarks."""
    analysis = {
        "overall_performance": {},
        "hardware_efficiency": {},
        "benchmark_rankings": {}
    }
    
    # Overall performance metrics
    accuracies = [result.accuracy for result in results.values()]
    latencies = [result.latency_ns for result in results.values()]
    powers = [result.power_consumption_mw for result in results.values()]
    
    analysis["overall_performance"] = {
        "avg_accuracy": np.mean(accuracies),
        "avg_latency_ns": np.mean(latencies),
        "avg_power_mw": np.mean(powers),
        "min_latency_ns": np.min(latencies),
        "max_accuracy": np.max(accuracies)
    }
    
    # Hardware efficiency
    speedups = [result.speedup_vs_gpu for result in results.values()]
    energy_efficiencies = [result.energy_efficiency_vs_gpu for result in results.values()]
    
    analysis["hardware_efficiency"] = {
        "avg_speedup_vs_gpu": np.mean(speedups),
        "avg_energy_efficiency_vs_gpu": np.mean(energy_efficiencies),
        "total_energy_savings": np.sum(energy_efficiencies)
    }
    
    # Rankings
    benchmark_names = list(results.keys())
    accuracy_ranking = sorted(benchmark_names, key=lambda x: results[x].accuracy, reverse=True)
    speed_ranking = sorted(benchmark_names, key=lambda x: results[x].latency_ns)
    
    analysis["benchmark_rankings"] = {
        "accuracy_ranking": accuracy_ranking,
        "speed_ranking": speed_ranking
    }
    
    return analysis